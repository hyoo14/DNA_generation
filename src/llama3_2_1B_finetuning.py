

# !pip install -q transformers peft accelerate trl datasets scipy bitsandbytes

import torch
from datasets import Dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from trl import SFTTrainer
from peft import LoraConfig, get_peft_model
import pandas as pd

#  Hugging Faceì—ì„œ Llama 3.2 1B ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
model_name = "meta-llama/Llama-3.2-1B"

#  ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
max_seq_length = 4096  # ë˜ëŠ” 5020
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, token=HF_TOKEN)
tokenizer.pad_token = tokenizer.eos_token  # PAD í† í° ì„¤ì •
tokenizer.padding_side = "right"
tokenizer.model_max_length = max_seq_length  #  ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ ì„¤ì •

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",  # ìë™ìœ¼ë¡œ GPU í• ë‹¹
    token=HF_TOKEN,
    trust_remote_code=True
)

#  LoRA ì„¤ì •
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()  # í•™ìŠµë˜ëŠ” íŒŒë¼ë¯¸í„° ìˆ˜ í™•ì¸

#  CSV ë°ì´í„° ë¡œë“œ ë° ê°€ê³µ
train_df = pd.read_csv("/content/drive/MyDrive/gen_dataset_gen/csv_datasets/train_task.csv")
test_df = pd.read_csv("/content/drive/MyDrive/gen_dataset_gen/csv_datasets/test_task.csv")

train_df["text"] = "Input: " + train_df["input_sequence"] + " Output: " + train_df["output_sequence"].apply(lambda x: x[:3000]) #[:3000]
test_df["text"] = "Input: " + test_df["input_sequence"] + " Output: "

train_dataset = Dataset.from_pandas(train_df[["text"]])
test_dataset = Dataset.from_pandas(test_df[["text"]])

#  ë°ì´í„° í† í°í™” í•¨ìˆ˜
def tokenize_function(example):
    return tokenizer(example["text"], padding="max_length", truncation=True, max_length=max_seq_length)

train_dataset = train_dataset.map(tokenize_function, batched=True)
test_dataset = test_dataset.map(tokenize_function, batched=True)

#  TrainingArguments ì„¤ì •
training_args = TrainingArguments(
    output_dir="./llama3_output",
    per_device_train_batch_size=2,  # OOM ë°©ì§€
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=8,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-4,
    weight_decay=0.01,
    num_train_epochs=3,
    logging_dir="./logs",
    push_to_hub=False,
    save_total_limit=2,
    fp16=False,
    bf16=True,
    optim="paged_adamw_32bit",
    report_to="none"
)

#  SFTTrainer ì„¤ì • ë° í•™ìŠµ ì‹œì‘ (max_seq_length ì œê±°ë¨!)
trainer = SFTTrainer(
    model=model,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    args=training_args
)

print(" Training ì‹œì‘!")
trainer.train()


from huggingface_hub import notebook_login

import os
os.environ["HF_TOKEN"] = HF_TOKEN#"hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"

from huggingface_hub import login
login(os.getenv("HF_TOKEN"))  # ì´ ë°©ë²•ì´ ê³µì‹ì ìœ¼ë¡œ ì§€ì›ë¨!


from huggingface_hub import HfApi

api = HfApi()
user_info = api.whoami()
print(f"Hugging Face ë¡œê·¸ì¸ ì„±ê³µ! ê³„ì •: {user_info['name']}")


from huggingface_hub import HfApi

repo_name = "Llama-3.2-1B-dna_gen"  # ì›í•˜ëŠ” ì €ì¥ì†Œ ì´ë¦„
api = HfApi()
api.create_repo(repo_name, private=False)  #  private=True í•˜ë©´ ë¹„ê³µê°œ ë ˆí¬!


from transformers import AutoModelForCausalLM, AutoTokenizer

repo_name = "hyoo14/Llama-3.2-1B-dna_gen"  # ë‚´ ì›ë³¸ ëª¨ë¸ ì´ë¦„

# ëª¨ë¸ ì €ì¥ (Colab ê¸°ì¤€)
save_directory = "/content/llama3_finetuned"

# ëª¨ë¸ ì €ì¥
model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

# Hugging Face ì—…ë¡œë“œ
model.push_to_hub(repo_name)
tokenizer.push_to_hub(repo_name)



from transformers import AutoModelForCausalLM, AutoTokenizer

repo_name = "hyoo14/Llama-3.2-1B-dna_gen"  # ğŸš€ ë„¤ê°€ ì—…ë¡œë“œí•œ Hugging Face ëª¨ë¸

model_hf = AutoModelForCausalLM.from_pretrained(repo_name)
tokenizer_hf = AutoTokenizer.from_pretrained(repo_name)


import torch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_hf.to(device)


import pandas as pd
test_df = pd.read_csv("/content/drive/MyDrive/gen_dataset_gen/csv_datasets/test_task.csv")

import torch

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ 2ê°œ ìƒ˜í”Œ ì„ íƒ
sample_inputs = test_df["input_sequence"][:2].tolist()
sample_outputs = test_df["output_sequence"][:2].tolist()  # ì •ë‹µ ë¹„êµìš©

# í”„ë¡¬í”„íŠ¸ í˜•ì‹ ë§ì¶”ê¸° (ëª¨ë¸ ì…ë ¥ í˜•ì‹)
formatted_inputs = ["Input: " + inp + " Output: " for inp in sample_inputs]


# í† í¬ë‚˜ì´ì§•
inputs = tokenizer_hf(formatted_inputs, return_tensors="pt", padding=True, truncation=True, max_length=4096).to("cuda")

# Greedy Decoding ë°©ì‹ìœ¼ë¡œ ìƒì„±
with torch.no_grad():
    outputs = model_hf.generate(
        input_ids=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=3000,  # ì¶œë ¥ ìµœëŒ€ ê¸¸ì´
        temperature=0.0,  # greedy decoding (ë¬´ì‘ìœ„ì„± ì—†ìŒ)
        do_sample=False  # ìƒ˜í”Œë§ ë¹„í™œì„±í™” (í•­ìƒ ê°™ì€ ì¶œë ¥)
    )

# ë””ì½”ë”©
generated_texts = tokenizer_hf.batch_decode(outputs, skip_special_tokens=True)

# ì¶œë ¥ ê²°ê³¼ ë¹„êµ
for i in range(2):
    print(f"\n **Example {i+1}**")
    print(f"**Input:** {sample_inputs[i]}")
    print(f" **Generated Output:** {generated_texts[i]}")
    print(f" **Expected Output:** {sample_outputs[i]}")


# !pip install Levenshtein

model_and_task = "llama_ft"

import torch
import pandas as pd
from transformers import AutoModelForCausalLM, AutoTokenizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import Levenshtein

# ìœ ì‚¬ë„ ë° GC Content ê³„ì‚° í•¨ìˆ˜
def jaccard_similarity(seq1, seq2):
    set1, set2 = set(seq1), set(seq2)
    return len(set1 & set2) / len(set1 | set2) if set1 | set2 else 0

def cosine_similarity_custom(seq1, seq2):
    vectorizer = CountVectorizer(analyzer='char').fit([seq1, seq2])
    vectors = vectorizer.transform([seq1, seq2]).toarray()
    return cosine_similarity([vectors[0]], [vectors[1]])[0][0]

def levenshtein_similarity(seq1, seq2):
    return 1 - (Levenshtein.distance(seq1, seq2) / max(len(seq1), len(seq2)))

def gc_content(sequence):
    return (sequence.count("G") + sequence.count("C")) / len(sequence) if sequence else 0

# ê²°ê³¼ ì €ì¥ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸
results = []

# ëª¨ë“  test_df ë°ì´í„°ì— ëŒ€í•´ ì‹¤í–‰
batch_size = 2  # í•œ ë²ˆì— ì²˜ë¦¬í•  ê°œìˆ˜ (ì¡°ì • ê°€ëŠ¥)
for i in range(0, len(test_df), batch_size):
#for i in range(0, 2, batch_size):
    batch = test_df.iloc[i : i + batch_size]  # batch ë‹¨ìœ„ë¡œ ë°ì´í„° ì²˜ë¦¬

    sample_inputs = batch["input_sequence"].tolist()
    sample_outputs = batch["output_sequence"].tolist()  # ì •ë‹µ ë¹„êµìš©

    # í”„ë¡¬í”„íŠ¸ í˜•ì‹ ë§ì¶”ê¸°
    formatted_inputs = ["Input: " + inp + " Output: " for inp in sample_inputs]

    #  ì…ë ¥ ì‹œí€€ìŠ¤ í† í°í™” ë° GPUë¡œ ì´ë™
    inputs = tokenizer_hf(
        formatted_inputs, return_tensors="pt", padding=True, truncation=True, max_length=4096
    ).to("cuda")

    #  DNA ì‹œí€€ìŠ¤ ìƒì„±
    with torch.no_grad():
        outputs = model_hf.generate(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_new_tokens=3000,  # ì¶œë ¥ ìµœëŒ€ ê¸¸ì´
            do_sample=False  # Greedy decoding (ìƒ˜í”Œë§ ë¹„í™œì„±í™”)
        )

    #  ìƒì„±ëœ ì‹œí€€ìŠ¤ë¥¼ ë””ì½”ë”©
    generated_texts = tokenizer_hf.batch_decode(outputs, skip_special_tokens=True)

    #  ìœ ì‚¬ë„ ë° GC Content ê³„ì‚°
    for j in range(len(sample_inputs)):
        generated_seq = generated_texts[j][:3000]  # 3000bp ì œí•œ
        expected_seq = sample_outputs[j][:3000]

        jaccard = jaccard_similarity(generated_seq, expected_seq)
        cosine = cosine_similarity_custom(generated_seq, expected_seq)
        levenshtein = levenshtein_similarity(generated_seq, expected_seq)
        gc_generated = gc_content(generated_seq)
        gc_expected = gc_content(expected_seq)

        # 7ï¸âƒ£ ê²°ê³¼ ì €ì¥
        results.append([
            sample_inputs[j], generated_seq, expected_seq, jaccard, cosine, levenshtein, gc_generated, gc_expected
        ])

    # ì§„í–‰ ìƒíƒœ ì¶œë ¥
    print(f"Processed {min(i + batch_size, len(test_df))}/{len(test_df)} sequences")

#  DataFrame ìƒì„± ë° CSV ì €ì¥
df_result = pd.DataFrame(results, columns=[
    "input_sequence", "generated_sequence", "expected_output",
    "jaccard_similarity", "cosine_similarity", "levenshtein_similarity",
    "gc_generated", "gc_expected"
])

# Google Driveì— ì €ì¥
csv_path = f"/content/drive/MyDrive/gen_dataset_gen/results/{model_and_task}_generated_sequences_greedy_all.csv"
df_result.to_csv(csv_path, index=False)

#  í‰ê·  ìœ ì‚¬ë„ ì¶œë ¥
mean_jaccard = df_result["jaccard_similarity"].mean()
mean_cosine = df_result["cosine_similarity"].mean()
mean_levenshtein = df_result["levenshtein_similarity"].mean()
print(f"\n Jaccard Similarity Mean: {mean_jaccard:.4f}")
print(f" Cosine Similarity Mean: {mean_cosine:.4f}")
print(f" Levenshtein Similarity Mean: {mean_levenshtein:.4f}")

#  ğŸ”Ÿ GC content ìƒê´€ê´€ê³„ ë¶„ì„
correlation = df_result[["gc_generated", "gc_expected"]].corr()
print("\n GC Content Correlation:")
print(correlation)

# ê²°ê³¼ ì €ì¥ ì™„ë£Œ ë©”ì‹œì§€
print(f"\n Results saved to: {csv_path}")




